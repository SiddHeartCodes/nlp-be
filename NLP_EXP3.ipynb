{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45b5191",
   "metadata": {},
   "source": [
    "\n",
    "# Word2Vec Word Embedding Experiment\n",
    "\n",
    "This experiment demonstrates how to generate word embeddings using the Word2Vec model for text similarity analysis. We'll use a sample corpus, preprocess the text, train the model, and visualize the resulting embeddings using PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040fcab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install gensim nltk matplotlib scikit-learn\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "corpus = [\n",
    "    \"Machine learning is a method of data analysis that automates analytical model building.\",\n",
    "    \"Artificial intelligence is the simulation of human intelligence in machines.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\",\n",
    "    \"Word2Vec is one of the most popular techniques to learn word embeddings using shallow neural networks.\"\n",
    "]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [t for t in tokens if t not in stop_words and t not in string.punctuation]\n",
    "\n",
    "tokenized_corpus = [preprocess(sentence) for sentence in corpus]\n",
    "tokenized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = list(model.wv.index_to_key)\n",
    "word_vectors = [model.wv[word] for word in words]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(word_vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "plt.title('Word2Vec Word Embeddings Visualization (PCA)')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
